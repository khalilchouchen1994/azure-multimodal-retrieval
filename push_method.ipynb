{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Asynchronous Indexing Pipeline with Text and Image Embeddings  \n",
    "   \n",
    "This notebook demonstrates how to create a **custom asynchronous indexing pipeline** that:  \n",
    "   \n",
    "- Reads PDF documents from Azure Blob Storage.  \n",
    "- Extracts text and images using Azure Document Intelligence.  \n",
    "- Generates embeddings for text and images using **Cohere models** in **Azure AI Foundry**.  \n",
    "- Indexes the data into **Azure AI Search** with separate `text_vector` and `image_vector` fields.  \n",
    "- Allows searching over text and image vectors.  \n",
    "   \n",
    "We will go through the following steps:  \n",
    "   \n",
    "1. **Install Required Libraries**  \n",
    "2. **Set Up Environment Variables**  \n",
    "3. **Create the Azure AI Search Index**  \n",
    "4. **Define the Custom Indexing Pipeline Components**  \n",
    "5. **Initialize and Run the Indexing Pipeline**  \n",
    "6. **Perform Test Searches**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install Required Libraries\n",
    "\n",
    "Make sure to install the necessary Python packages. Execute the following cells to install them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Install required packages  \n",
    "%pip install azure-search-documents==11.6.0b4 \n",
    "%pip install azure-core azure-storage-blob azure-ai-document-intelligence  \n",
    "%pip install azure-ai-ml  \n",
    "%pip install azure-ai-inference  \n",
    "%pip install openai  \n",
    "%pip install aiohttp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries and configure logging  \n",
    "   \n",
    "import logging  \n",
    "   \n",
    "# Configure the root logger  \n",
    "logging.basicConfig(  \n",
    "    level=logging.INFO,  # Set root logger level  \n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'  \n",
    ")  \n",
    "   \n",
    "# Suppress logs from azure and uamqp libraries  \n",
    "logging.getLogger('azure').setLevel(logging.WARNING)  \n",
    "logging.getLogger('uamqp').setLevel(logging.WARNING)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Set Up Environment Variables  \n",
    "   \n",
    "Set up the necessary environment variables by creating a `.env` file or by setting them directly in the notebook. Avoid hardcoding sensitive information in the notebook.  \n",
    "   \n",
    "Required environment variables:  \n",
    "   \n",
    "- `AZURE_SEARCH_SERVICE_ENDPOINT`  \n",
    "- `AZURE_SEARCH_API_KEY`  \n",
    "- `AZURE_STORAGE_ACCOUNT_NAME`  \n",
    "- `AZURE_STORAGE_ACCOUNT_CONTAINER_NAME`  \n",
    "- `AZURE_OPENAI_ENDPOINT`  \n",
    "- `AZURE_OPENAI_KEY`  \n",
    "- `AZURE_OPENAI_EMBEDDING_DEPLOYMENT`  \n",
    "- `AZURE_OPENAI_EMBEDDING_MODEL_NAME`  \n",
    "- `AZURE_OPENAI_EMBEDDING_DIMENSIONS`  \n",
    "- `AZURE_AI_INFERENCE_ENDPOINT`  \n",
    "- `AZURE_AI_INFERENCE_KEY`  \n",
    "- `DOCUMENTINTELLIGENCE_ENDPOINT`  \n",
    "- `DOCUMENTINTELLIGENCE_API_KEY`  \n",
    "   \n",
    "Ensure that the identities used have the necessary permissions (e.g., **Storage Blob Data Reader** role).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  \n",
    "from dotenv import load_dotenv  \n",
    "   \n",
    "# Load environment variables from .env file  \n",
    "load_dotenv(override=True)  \n",
    "   \n",
    "# Azure Cognitive Search settings  \n",
    "search_service_endpoint = os.environ[\"AZURE_SEARCH_SERVICE_ENDPOINT\"]  \n",
    "search_api_key = os.environ[\"AZURE_SEARCH_API_KEY\"]  \n",
    "index_name = \"asynch-custom-push-demo\"  \n",
    "   \n",
    "# Azure Storage settings  \n",
    "storage_account_name = os.environ[\"AZURE_STORAGE_ACCOUNT_NAME\"]  \n",
    "storage_container_name = os.environ[\"AZURE_STORAGE_ACCOUNT_CONTAINER_NAME\"]  \n",
    "   \n",
    "# Azure OpenAI settings (for text embeddings)\n",
    "# text_embedding_endpoint = os.environ[\"AZURE_OPENAI_ENDPOINT\"]  \n",
    "# text_embedding_key = os.environ[\"AZURE_OPENAI_KEY\"]  \n",
    "# text_embedding_model_name = os.environ[\"AZURE_OPENAI_EMBEDDING_MODEL_NAME\"]  \n",
    "# text_embedding_dimensions = int(os.getenv(\"AZURE_OPENAI_EMBEDDING_DIMENSIONS\", 1536))  \n",
    "\n",
    "\n",
    " \n",
    "   \n",
    "# Azure AI Inference settings (for embeddings)  \n",
    "ai_foundry_endpoint = os.environ[\"AZURE_AI_FOUNDRY_ENDPOINT\"]  \n",
    "ai_foundry_key = os.environ[\"AZURE_AI_FOUNDRY_KEY\"] \n",
    "text_embedding_model = os.environ[\"TEXT_EMBEDDING_MODEL\"]\n",
    "text_embedding_dimensions = int(os.getenv(\"TEXT_EMBEDDING_DIMENSIONS\", 1024)) \n",
    "image_embedding_model= os.environ[\"IMAGE_EMBEDDING_MODEL\"]\n",
    "image_embedding_dimensions =  int(os.getenv(\"IMAGE_EMBEDDING_DIMENSIONS\", 1024))    # Set this based on your image embedding model  \n",
    "   \n",
    "# Azure Document Intelligence settings  \n",
    "document_intelligence_endpoint = os.environ[\"DOCUMENTINTELLIGENCE_ENDPOINT\"]  \n",
    "document_intelligence_key = os.environ[\"DOCUMENTINTELLIGENCE_API_KEY\"]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create the Azure AI Search Index  \n",
    "   \n",
    "We'll create the search index with the appropriate schema, including separate fields for `text_vector` and `image_vector`, and a `page_number` field to track the pages.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index 'asynch-custom-push-demo' created or updated.\n"
     ]
    }
   ],
   "source": [
    "from azure.search.documents.indexes import SearchIndexClient  \n",
    "from azure.search.documents.indexes.models import (  \n",
    "    SearchField,  \n",
    "    SearchFieldDataType,  \n",
    "    VectorSearch,  \n",
    "    HnswAlgorithmConfiguration,  \n",
    "    VectorSearchProfile,  \n",
    "    SemanticConfiguration,  \n",
    "    SemanticSearch,  \n",
    "    SemanticPrioritizedFields,  \n",
    "    SemanticField,  \n",
    "    SearchIndex  \n",
    ")  \n",
    "\n",
    "from azure.core.credentials import AzureKeyCredential  \n",
    "   \n",
    "# Create a SearchIndexClient  \n",
    "search_index_client = SearchIndexClient(  \n",
    "    endpoint=search_service_endpoint,  \n",
    "    credential=AzureKeyCredential(search_api_key)  \n",
    ")  \n",
    "   \n",
    "# Define the index schema  \n",
    "fields = [  \n",
    "    SearchField(  \n",
    "        name=\"parent_id\",  \n",
    "        type=SearchFieldDataType.String,  \n",
    "        filterable=True,  \n",
    "        facetable=True,  \n",
    "        sortable=True  \n",
    "    ),  \n",
    "    SearchField(  \n",
    "        name=\"chunk_id\",  \n",
    "        type=SearchFieldDataType.String,  \n",
    "        key=True,  \n",
    "        filterable=True,  \n",
    "        facetable=True,  \n",
    "        sortable=True  \n",
    "    ),  \n",
    "    SearchField(  \n",
    "        name=\"title\",  \n",
    "        type=SearchFieldDataType.String,  \n",
    "        filterable=True,  \n",
    "        facetable=True,  \n",
    "        sortable=True  \n",
    "    ),  \n",
    "    SearchField(  \n",
    "        name=\"chunk\",  \n",
    "        type=SearchFieldDataType.String,  \n",
    "        searchable=True  \n",
    "    ),  \n",
    "    SearchField(  \n",
    "        name=\"text_vector\",  \n",
    "        type=SearchFieldDataType.Collection(SearchFieldDataType.Single),  \n",
    "        vector_search_dimensions=text_embedding_dimensions,  \n",
    "        vector_search_profile_name=\"textHnswProfile\",  \n",
    "    ),  \n",
    "    SearchField(  \n",
    "        name=\"image_vector\",  \n",
    "        type=SearchFieldDataType.Collection(SearchFieldDataType.Single),  \n",
    "        vector_search_dimensions=image_embedding_dimensions,  \n",
    "        vector_search_profile_name=\"imageHnswProfile\",  \n",
    "    ),  \n",
    "    SearchField(  \n",
    "        name=\"page_number\",  \n",
    "        type=SearchFieldDataType.Int32,  \n",
    "        filterable=True,  \n",
    "        facetable=True,  \n",
    "        sortable=True  \n",
    "    ),  \n",
    "]  \n",
    "   \n",
    "# Configure the vector search settings  \n",
    "vector_search = VectorSearch(  \n",
    "    algorithms=[  \n",
    "        HnswAlgorithmConfiguration(name=\"myHnswAlgorithm\")  \n",
    "    ],  \n",
    "    profiles=[  \n",
    "        VectorSearchProfile(  \n",
    "            name=\"textHnswProfile\",  \n",
    "            algorithm_configuration_name=\"myHnswAlgorithm\",  \n",
    "        ),  \n",
    "        VectorSearchProfile(  \n",
    "            name=\"imageHnswProfile\",  \n",
    "            algorithm_configuration_name=\"myHnswAlgorithm\",  \n",
    "        ),  \n",
    "    ],  \n",
    ")  \n",
    "   \n",
    "# Configure semantic search settings (optional)  \n",
    "semantic_config = SemanticConfiguration(  \n",
    "    name=\"my-semantic-config\",  \n",
    "    prioritized_fields=SemanticPrioritizedFields(  \n",
    "        title_field=SemanticField(field_name=\"title\"),  \n",
    "        content_fields=[SemanticField(field_name=\"chunk\")],  \n",
    "    ),  \n",
    ")  \n",
    "   \n",
    "semantic_search = SemanticSearch(configurations=[semantic_config])  \n",
    "   \n",
    "# Create the search index  \n",
    "index = SearchIndex(  \n",
    "    name=index_name,  \n",
    "    fields=fields,  \n",
    "    vector_search=vector_search,  \n",
    "    semantic_search=semantic_search,  \n",
    ")  \n",
    "   \n",
    "# Create or update the index in Azure Cognitive Search  \n",
    "search_index_client.create_or_update_index(index)  \n",
    "   \n",
    "print(f\"Index '{index.name}' created or updated.\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define the Custom Indexing Pipeline Components  \n",
    "   \n",
    "The custom indexing pipeline consists of the following components:  \n",
    "   \n",
    "- **FileReader**: Reads PDFs using Azure Document Intelligence and extracts text and images.  \n",
    "- **Chunker**: Splits the text into chunks for embedding.  \n",
    "- **TextEmbedder**: Generates text embeddings using Azure OpenAI.  \n",
    "- **ImageEmbedder**: Generates image embeddings using Azure AI Inference.  \n",
    "- **FileUploader**: Uploads the processed documents into Azure Cognitive Search.  \n",
    "- **AsynchronousIndexer**: Orchestrates the entire pipeline asynchronously.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Initialize and Run the Indexing Pipeline  \n",
    "   \n",
    "Now we'll initialize the `AsynchronousIndexer` with the appropriate settings and run the indexing pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-28 12:40:25,476 - asynch_indexer.AsynchronousIndexer - INFO - Reader read_worker_0: Reading document 1_London_Brochure.pdf\n",
      "2024-12-28 12:40:25,480 - asynch_indexer.AsynchronousIndexer - INFO - Reader read_worker_1: Reading document 1_slide_1.pdf\n",
      "2024-12-28 12:40:30,225 - asynch_indexer.AsynchronousIndexer - INFO - Reader read_worker_1: Completed analyze_document for 1_slide_1.pdf\n",
      "2024-12-28 12:40:30,323 - asynch_indexer.AsynchronousIndexer - INFO - Reader read_worker_0: Completed analyze_document for 1_London_Brochure.pdf\n",
      "2024-12-28 12:40:30,838 - asynch_indexer.AsynchronousIndexer - INFO - Reader read_worker_1: Found 4 figures in 1_slide_1.pdf\n",
      "2024-12-28 12:40:30,992 - asynch_indexer.AsynchronousIndexer - INFO - Reader read_worker_1: Finished reading 1_slide_1.pdf in 5.51 seconds\n",
      "2024-12-28 12:40:30,993 - asynch_indexer.AsynchronousIndexer - INFO - Reader read_worker_1: Done processing 1_slide_1.pdf\n",
      "2024-12-28 12:40:30,994 - asynch_indexer.AsynchronousIndexer - INFO - Chunker chunk_worker_0: Chunking document 1_slide_1.pdf page 1\n",
      "2024-12-28 12:40:30,995 - asynch_indexer.AsynchronousIndexer - INFO - Chunker chunk_worker_0: Finished chunking in 0.00 seconds\n",
      "2024-12-28 12:40:30,996 - asynch_indexer.AsynchronousIndexer - INFO - ImageEmbedder image_embedder_0:Embedding image 2d1e76a6-417a-40b9-814a-03c19158df39 from document 1_slide_1.pdf page 1\n",
      "2024-12-28 12:40:32,389 - asynch_indexer.AsynchronousIndexer - INFO - ImageEmbedder image_embedder_0: Successfully processed image 2d1e76a6-417a-40b9-814a-03c19158df39 using model embed-multilingual-v3.0-image. Token consumption {'prompt_tokens': 1000, 'completion_tokens': 0, 'total_tokens': 1000, 'images': 1}\n",
      "2024-12-28 12:40:32,390 - asynch_indexer.AsynchronousIndexer - INFO - ImageEmbedder image_embedder_0:Embedding image 34172381-6c48-4644-a607-cb27a998eba3 from document 1_slide_1.pdf page 1\n",
      "2024-12-28 12:40:33,240 - asynch_indexer.AsynchronousIndexer - INFO - ImageEmbedder image_embedder_0: Successfully processed image 34172381-6c48-4644-a607-cb27a998eba3 using model embed-multilingual-v3.0-image. Token consumption {'prompt_tokens': 1000, 'completion_tokens': 0, 'total_tokens': 1000, 'images': 1}\n",
      "2024-12-28 12:40:33,241 - asynch_indexer.AsynchronousIndexer - INFO - ImageEmbedder image_embedder_0:Embedding image f4c011f1-91c7-4bfa-b189-0b843c3211fb from document 1_slide_1.pdf page 1\n",
      "2024-12-28 12:40:33,544 - asynch_indexer.AsynchronousIndexer - INFO - ImageEmbedder image_embedder_0: Successfully processed image f4c011f1-91c7-4bfa-b189-0b843c3211fb using model embed-multilingual-v3.0-image. Token consumption {'prompt_tokens': 1000, 'completion_tokens': 0, 'total_tokens': 1000, 'images': 1}\n",
      "2024-12-28 12:40:33,545 - asynch_indexer.AsynchronousIndexer - INFO - ImageEmbedder image_embedder_0:Embedding image 8b3dab3c-717a-4ca9-aa00-f398d0a62929 from document 1_slide_1.pdf page 1\n",
      "2024-12-28 12:40:33,906 - asynch_indexer.AsynchronousIndexer - INFO - ImageEmbedder image_embedder_0: Successfully processed image 8b3dab3c-717a-4ca9-aa00-f398d0a62929 using model embed-multilingual-v3.0-image. Token consumption {'prompt_tokens': 1000, 'completion_tokens': 0, 'total_tokens': 1000, 'images': 1}\n",
      "2024-12-28 12:40:33,907 - asynch_indexer.AsynchronousIndexer - INFO - TextEmbedder text_embedder_0: Embedding chunk 82d5e7d1-00b6-4f52-bf35-6ba35b22124d from document 1_slide_1.pdf page 1\n",
      "2024-12-28 12:40:34,736 - asynch_indexer.AsynchronousIndexer - INFO - TextEmbedder text_embedder_0: Finished embedding in 0.83 seconds\n",
      "2024-12-28 12:40:34,737 - asynch_indexer.AsynchronousIndexer - INFO - Uploader uploader_0: Uploading chunk 2d1e76a6-417a-40b9-814a-03c19158df39 of document fcc5c953-d557-4b2e-8c93-0331148b7db3\n",
      "2024-12-28 12:40:34,739 - asynch_indexer.AsynchronousIndexer - INFO - Uploader uploader_0: Successfully uploaded 2d1e76a6-417a-40b9-814a-03c19158df39\n",
      "2024-12-28 12:40:34,740 - asynch_indexer.AsynchronousIndexer - INFO - Uploader uploader_0: Uploading chunk 34172381-6c48-4644-a607-cb27a998eba3 of document fcc5c953-d557-4b2e-8c93-0331148b7db3\n",
      "2024-12-28 12:40:34,741 - asynch_indexer.AsynchronousIndexer - INFO - Uploader uploader_0: Successfully uploaded 34172381-6c48-4644-a607-cb27a998eba3\n",
      "2024-12-28 12:40:34,742 - asynch_indexer.AsynchronousIndexer - INFO - Uploader uploader_0: Uploading chunk f4c011f1-91c7-4bfa-b189-0b843c3211fb of document fcc5c953-d557-4b2e-8c93-0331148b7db3\n",
      "2024-12-28 12:40:34,746 - asynch_indexer.AsynchronousIndexer - INFO - Uploader uploader_0: Successfully uploaded f4c011f1-91c7-4bfa-b189-0b843c3211fb\n",
      "2024-12-28 12:40:34,748 - asynch_indexer.AsynchronousIndexer - INFO - Uploader uploader_0: Uploading chunk 8b3dab3c-717a-4ca9-aa00-f398d0a62929 of document fcc5c953-d557-4b2e-8c93-0331148b7db3\n",
      "2024-12-28 12:40:34,749 - asynch_indexer.AsynchronousIndexer - INFO - Uploader uploader_0: Successfully uploaded 8b3dab3c-717a-4ca9-aa00-f398d0a62929\n",
      "2024-12-28 12:40:34,750 - asynch_indexer.AsynchronousIndexer - INFO - Uploader uploader_0: Uploading chunk 82d5e7d1-00b6-4f52-bf35-6ba35b22124d of document fcc5c953-d557-4b2e-8c93-0331148b7db3\n",
      "2024-12-28 12:40:34,751 - asynch_indexer.AsynchronousIndexer - INFO - Uploader uploader_0: Successfully uploaded 82d5e7d1-00b6-4f52-bf35-6ba35b22124d\n",
      "2024-12-28 12:40:34,752 - asynch_indexer.AsynchronousIndexer - INFO - Reader read_worker_0: Found 2 figures in 1_London_Brochure.pdf\n",
      "2024-12-28 12:40:35,434 - asynch_indexer.AsynchronousIndexer - INFO - Reader read_worker_0: Finished reading 1_London_Brochure.pdf in 9.96 seconds\n",
      "2024-12-28 12:40:35,435 - asynch_indexer.AsynchronousIndexer - INFO - Reader read_worker_0: Done processing 1_London_Brochure.pdf\n",
      "2024-12-28 12:40:35,436 - asynch_indexer.AsynchronousIndexer - INFO - Chunker chunk_worker_1: Chunking document 1_London_Brochure.pdf page 1\n",
      "2024-12-28 12:40:35,437 - asynch_indexer.AsynchronousIndexer - INFO - Chunker chunk_worker_1: Finished chunking in 0.00 seconds\n",
      "2024-12-28 12:40:35,437 - asynch_indexer.AsynchronousIndexer - INFO - Chunker chunk_worker_1: Chunking document 1_London_Brochure.pdf page 2\n",
      "2024-12-28 12:40:35,438 - asynch_indexer.AsynchronousIndexer - INFO - Chunker chunk_worker_1: Finished chunking in 0.00 seconds\n",
      "2024-12-28 12:40:35,439 - asynch_indexer.AsynchronousIndexer - INFO - ImageEmbedder image_embedder_0:Embedding image ae0c2a50-1a84-40cf-b00a-7d42bc67c483 from document 1_London_Brochure.pdf page 1\n",
      "2024-12-28 12:40:38,190 - asynch_indexer.AsynchronousIndexer - INFO - ImageEmbedder image_embedder_0: Successfully processed image ae0c2a50-1a84-40cf-b00a-7d42bc67c483 using model embed-multilingual-v3.0-image. Token consumption {'prompt_tokens': 1000, 'completion_tokens': 0, 'total_tokens': 1000, 'images': 1}\n",
      "2024-12-28 12:40:38,192 - asynch_indexer.AsynchronousIndexer - INFO - ImageEmbedder image_embedder_0:Embedding image 41dd1521-fd16-4f69-a0d3-355a920aa157 from document 1_London_Brochure.pdf page 1\n",
      "2024-12-28 12:40:39,450 - asynch_indexer.AsynchronousIndexer - INFO - ImageEmbedder image_embedder_0: Successfully processed image 41dd1521-fd16-4f69-a0d3-355a920aa157 using model embed-multilingual-v3.0-image. Token consumption {'prompt_tokens': 1000, 'completion_tokens': 0, 'total_tokens': 1000, 'images': 1}\n",
      "2024-12-28 12:40:39,451 - asynch_indexer.AsynchronousIndexer - INFO - TextEmbedder text_embedder_1: Embedding chunk dc73d54d-1cf8-4477-b44c-75f42756e68d from document 1_London_Brochure.pdf page 1\n",
      "2024-12-28 12:40:39,681 - asynch_indexer.AsynchronousIndexer - INFO - TextEmbedder text_embedder_1: Finished embedding in 0.23 seconds\n",
      "2024-12-28 12:40:39,682 - asynch_indexer.AsynchronousIndexer - INFO - TextEmbedder text_embedder_1: Embedding chunk 009a29ec-6293-4566-8a49-dd79ff67e51f from document 1_London_Brochure.pdf page 2\n",
      "2024-12-28 12:40:39,905 - asynch_indexer.AsynchronousIndexer - INFO - TextEmbedder text_embedder_1: Finished embedding in 0.22 seconds\n",
      "2024-12-28 12:40:39,907 - asynch_indexer.AsynchronousIndexer - INFO - Uploader uploader_0: Uploading chunk ae0c2a50-1a84-40cf-b00a-7d42bc67c483 of document a7f741c6-957d-499c-8be2-f12f13e8f08a\n",
      "2024-12-28 12:40:39,908 - asynch_indexer.AsynchronousIndexer - INFO - Uploader uploader_0: Successfully uploaded ae0c2a50-1a84-40cf-b00a-7d42bc67c483\n",
      "2024-12-28 12:40:39,909 - asynch_indexer.AsynchronousIndexer - INFO - Uploader uploader_0: Uploading chunk 41dd1521-fd16-4f69-a0d3-355a920aa157 of document a7f741c6-957d-499c-8be2-f12f13e8f08a\n",
      "2024-12-28 12:40:39,910 - asynch_indexer.AsynchronousIndexer - INFO - Uploader uploader_0: Successfully uploaded 41dd1521-fd16-4f69-a0d3-355a920aa157\n",
      "2024-12-28 12:40:39,912 - asynch_indexer.AsynchronousIndexer - INFO - Uploader uploader_0: Uploading chunk dc73d54d-1cf8-4477-b44c-75f42756e68d of document a7f741c6-957d-499c-8be2-f12f13e8f08a\n",
      "2024-12-28 12:40:39,914 - asynch_indexer.AsynchronousIndexer - INFO - Uploader uploader_0: Successfully uploaded dc73d54d-1cf8-4477-b44c-75f42756e68d\n",
      "2024-12-28 12:40:39,915 - asynch_indexer.AsynchronousIndexer - INFO - Uploader uploader_0: Uploading chunk 009a29ec-6293-4566-8a49-dd79ff67e51f of document a7f741c6-957d-499c-8be2-f12f13e8f08a\n",
      "2024-12-28 12:40:39,916 - asynch_indexer.AsynchronousIndexer - INFO - Uploader uploader_0: Successfully uploaded 009a29ec-6293-4566-8a49-dd79ff67e51f\n",
      "2024-12-28 12:40:39,999 - asynch_indexer.AsynchronousIndexer - INFO - Total indexing time: 22.68 seconds\n"
     ]
    }
   ],
   "source": [
    "import nest_asyncio  \n",
    "import asyncio  \n",
    "from asynch_indexer.AsynchronousIndexer import AsynchronousIndexer  \n",
    "   \n",
    "# Necessary when running asyncio in Jupyter notebooks  \n",
    "nest_asyncio.apply()  \n",
    "   \n",
    "# Initialize the AsynchronousIndexer  \n",
    "indexer = AsynchronousIndexer(  \n",
    "    index_name=index_name,  \n",
    "    search_endpoint=search_service_endpoint,  \n",
    "    search_api_key=search_api_key,  \n",
    "    storage_account_name=storage_account_name,  \n",
    "    storage_container_name=storage_container_name,  \n",
    "    ai_foundry_endpoint = ai_foundry_endpoint  ,\n",
    "    ai_foundry_key = ai_foundry_key,\n",
    "    text_embedding_model= text_embedding_model,\n",
    "    image_embedding_model=image_embedding_model, \n",
    "    document_intelligence_endpoint=document_intelligence_endpoint,  \n",
    "    document_intelligence_key=document_intelligence_key,  \n",
    ")  \n",
    "   \n",
    "# Run the indexing pipeline  \n",
    "asyncio.run(indexer.run_indexing())  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Perform Test Searches  \n",
    "   \n",
    "Finally, we'll perform test searches against the index to verify that the text and image vectors have been indexed correctly.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: embed-multilingual-v3.0\n",
      "Usage: {'prompt_tokens': 6, 'completion_tokens': 0, 'total_tokens': 6}\n",
      "Title: 1_slide_1.pdf\n",
      "Page Number: 1\n",
      "Chunk: Infrastructure is forecasted to be one of the fastest-growing\n",
      "segments of private markets\n",
      "Industry infrastructure AUM1\n",
      "16%\n",
      "CAGR\n",
      "2,541\n",
      "10%\n",
      "CAGR\n",
      "1,190\n",
      "729\n",
      "2017\n",
      "2022\n",
      "2027\n",
      "Clients allocating more to infra in new market regime3\n",
      "Private Debt\n",
      "43%\n",
      "39%\n",
      "18%\n",
      "Infrastructure\n",
      "37%\n",
      "45%\n",
      "18%\n",
      "Private Equity\n",
      "28%\n",
      "57%\n",
      "15%\n",
      "Hedge Funds\n",
      "22%\n",
      "50%\n",
      "28%\n",
      "Venture Capital\n",
      "22%\n",
      "43%\n",
      "35%\n",
      "Real Estate\n",
      "18%\n",
      "52%\n",
      "30%\n",
      "More capital\n",
      "Same amount of capital\n",
      "Less capital\n",
      "Note: For footnoted information, refer to slide 11.\n",
      "$75T global infrastructure funding need2\n",
      "2022-2040 cumulative infrastructure investment & needs, $T\n",
      "Investment\n",
      "40\n",
      "Needs\n",
      "23\n",
      "7\n",
      "5\n",
      "Energy\n",
      "Telecom\n",
      "& digital\n",
      "Transport\n",
      "Water\n",
      "Infrastructure fares well in inflationary environments4\n",
      "High growth / high inflation\n",
      "20-year total returns (ann'd)\n",
      "17%\n",
      "16%\n",
      "15%\n",
      "0%\n",
      "Global Direct\n",
      "Infrastructure\n",
      "Global Direct\n",
      "Real Estate\n",
      "Global Equities\n",
      "Global Fixed\n",
      "Income\n",
      "23%\n",
      "Low growth / high inflation\n",
      "8%\n",
      "8%\n",
      "2%\n",
      "Global Direct\n",
      "Infrastructure\n",
      "Global Direct\n",
      "Real Estate\n",
      "Global Equities\n",
      "Global Fixed\n",
      "Income\n",
      "BlackRock.\n",
      "4\n",
      "---\n",
      "Title: 1_London_Brochure.pdf\n",
      "Page Number: 2\n",
      "Chunk: Category\n",
      "Information\n",
      "Country\n",
      "United Kingdom\n",
      "Capital Of\n",
      "England\n",
      "Currency\n",
      "Pound Sterling (GBP)\n",
      "Population (2021 census)\n",
      "Approximately 8.8 million\n",
      "Famous For\n",
      "Historical landmarks, museums, cultural diversity\n",
      "---\n",
      "Title: 1_London_Brochure.pdf\n",
      "Page Number: 1\n",
      "Chunk: Margie's Travel Presents ...\n",
      "London\n",
      "London is the capital and\n",
      "most populous city of\n",
      "England and the United\n",
      "Kingdom. Standing on the\n",
      "River Thames in the south\n",
      "east of the island of Great\n",
      "Britain, London has been\n",
      "a major settlement for two\n",
      "millennia. It was founded\n",
      "by the Romans, who\n",
      "named it Londinium.\n",
      "London's ancient core, the\n",
      "City of London, largely\n",
      "retains its 1.12-square-\n",
      "Tower Bridge\n",
      "mile medieval boundaries.\n",
      "Since at least the 19th century, London has also referred to the metropolis around this core,\n",
      "historically split between Middlesex, Essex, Surrey, Kent, and Hertfordshire, which today largely\n",
      "makes up Greater London, governed by the Mayor of London and the London Assembly.\n",
      "Fountains in Trafalgar Square\n",
      "Mostly popular for:\n",
      "Leisure, Outdoors, Historical, Arts\n",
      "& Culture\n",
      "Best time to visit:\n",
      "Jun-Aug\n",
      "Averag Precipitation: 1.9 in\n",
      "Average Temperature: 56-67ºF\n",
      "London Hotels\n",
      "Margie's Travel offers the following accommodation options in London:\n",
      "The Buckingham Hotel\n",
      "Comfortable hotel close to major sights like Buckingham Palace, Regent's Park, and Trafalgar\n",
      "Square.\n",
      "The City Hotel\n",
      "Luxury rooms in the city, within walking distance of Tower Bridge and the Tower of London ..\n",
      "The Kensington Hotel\n",
      "Budget accommodation near Earl's Court.\n",
      "To book your trip to London, visit www.margiestravel.com\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "from azure.search.documents import SearchClient\n",
    "from azure.ai.inference import EmbeddingsClient   \n",
    "from azure.search.documents.models import VectorizedQuery   \n",
    "   \n",
    "# Initialize the SearchClient  \n",
    "search_client = SearchClient(  \n",
    "    endpoint=search_service_endpoint,  \n",
    "    index_name=index_name,  \n",
    "    credential=AzureKeyCredential(search_api_key),  \n",
    ")\n",
    "\n",
    "embeddings_client = EmbeddingsClient(  \n",
    "        endpoint=ai_foundry_endpoint,  \n",
    "        credential=AzureKeyCredential(ai_foundry_key),  \n",
    "        model=text_embedding_model\n",
    ")  \n",
    "   \n",
    "# Define the search query  \n",
    "query_text = \"Enter your search query here\"  \n",
    "   \n",
    "# Function to get the query embedding using Coheremebed\n",
    "def get_query_embedding(query):  \n",
    "    response = embeddings_client.embed(  \n",
    "        input=[query]  \n",
    "    )  \n",
    "    print(\"Model:\", response.model)\n",
    "    print(\"Usage:\", response.usage)\n",
    "    return response.data[0].embedding\n",
    "   \n",
    "# Get the query embedding  \n",
    "query_embedding = get_query_embedding(query_text) \n",
    "   \n",
    "# Perform the vector search  \n",
    "vector_query = VectorizedQuery(  \n",
    "    vector=query_embedding,  \n",
    "    k_nearest_neighbors=3,  \n",
    "    fields=\"text_vector\", \n",
    ")  \n",
    "   \n",
    "results = search_client.search(  \n",
    "    search_text=query_embedding,  \n",
    "    vector_queries=[vector_query],  \n",
    "    select=[\"title\", \"chunk\", \"page_number\"],  \n",
    "    top=3  \n",
    ")  \n",
    "   \n",
    "# Print the results  \n",
    "for result in results:  \n",
    "    print(f\"Title: {result['title']}\")  \n",
    "    print(f\"Page Number: {result['page_number']}\")  \n",
    "    print(f\"Chunk: {result['chunk']}\")  \n",
    "    print(\"---\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion  \n",
    "   \n",
    "In this notebook, we've built a custom asynchronous indexing pipeline that processes both text and images from PDF documents, generates embeddings using Azure OpenAI and Azure AI Inference, and indexes them into Azure Cognitive Search. This allows for advanced vector-based searches over both text and images.  \n",
    "   \n",
    "You can extend this pipeline to handle more complex scenarios, larger datasets, or integrate additional processing steps as needed.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.core.credentials import AzureKeyCredential  \n",
    "from azure.ai.documentintelligence import DocumentIntelligenceClient  \n",
    "from azure.identity.aio import DefaultAzureCredential  \n",
    "from azure.storage.blob.aio import BlobServiceClient  \n",
    "\n",
    "from pdf2image import convert_from_bytes  \n",
    "from io import BytesIO \n",
    "# Display the image in the notebook  \n",
    "from IPython.display import display  \n",
    "\n",
    "\n",
    "from azure.ai.inference import EmbeddingsClient\n",
    "from azure.ai.inference import ImageEmbeddingsClient\n",
    "from azure.ai.inference.models import EmbeddingInput\n",
    "import base64  \n",
    "import io  \n",
    "from PIL import Image\n",
    "\n",
    "storage_account_url = f\"https://demosharedstorage1.blob.core.windows.net\"  \n",
    "storage_container_name = \"demo-indexer-storage\"\n",
    "credential = DefaultAzureCredential()  \n",
    "\n",
    "blob_service_client = BlobServiceClient(  \n",
    "            account_url=storage_account_url,  \n",
    "            credential=credential  \n",
    "        )  \n",
    "\n",
    "storage_container_client = blob_service_client.get_container_client(storage_container_name)  \n",
    "\n",
    "\n",
    "blob_list = storage_container_client.list_blobs()  \n",
    "  \n",
    "# Populate the file queue with blobs  \n",
    "async for blob in blob_list:  \n",
    "    if blob.name.endswith('.pdf'): \n",
    "        print(blob.name)\n",
    "        blob_client = storage_container_client.get_blob_client(blob)  \n",
    "        # Download the blob data asynchronously  \n",
    "        stream = await blob_client.download_blob()  \n",
    "        data = await stream.readall()\n",
    "\n",
    "        document_client = DocumentIntelligenceClient(  \n",
    "                    endpoint=document_intelligence_endpoint,  \n",
    "                    credential=AzureKeyCredential(document_intelligence_key) ,\n",
    "                ) \n",
    "        print(\"begin\")\n",
    "        poller = document_client.begin_analyze_document(  \n",
    "                    model_id=\"prebuilt-layout\",  \n",
    "                    body=data \n",
    "                )  \n",
    "        \n",
    "        result = poller.result()\n",
    "\n",
    "        text_pages = []  \n",
    "        images = []  \n",
    "        for page in result.pages: \n",
    "            page_text = \"\\n\".join([line.content for line in page.lines])  \n",
    "            text_pages.append((blob.name, page_text, \"1\", page.page_number))  \n",
    "\n",
    "\n",
    "        # Convert the PDF pages to images  \n",
    "\n",
    "        dpi = 300  # Adjust DPI as needed  \n",
    "        poppler_path= r\"C:\\Users\\kchouchen\\Desktop\\AI-search-multimodality-ip\\.venv\\Lib\\site-packages\\poppler-24.08.0\\Library\\bin\"\n",
    "        pages_images = convert_from_bytes(data, dpi=dpi, poppler_path =poppler_path)  \n",
    "        print(\"test\")\n",
    "        # Map page numbers to page objects  \n",
    "        pages_by_number = {page.page_number: page for page in result.pages}  \n",
    "        # Extract images from the page   \n",
    "        if result.figures:  \n",
    "            for figure in result.figures:  \n",
    "                for region in figure.bounding_regions:  \n",
    "                    page_number = region.page_number  \n",
    "                    polygon = region.polygon  # List of x, y coordinates  \n",
    "\n",
    "                    # Get the corresponding page image  \n",
    "                    page_image = pages_images[page_number - 1]  \n",
    "\n",
    "                    # Get the page object to determine units  \n",
    "                    page = pages_by_number[page_number]  \n",
    "                    unit = page.unit  # 'pixel' or 'inch'  \n",
    "\n",
    "                    # Map the polygon coordinates to pixel values  \n",
    "                    x_coords = polygon[0::2]  \n",
    "                    y_coords = polygon[1::2]  \n",
    "\n",
    "                    if unit == 'pixel':  \n",
    "                        # Coordinates are already in pixels  \n",
    "                        pass  \n",
    "                    elif unit == 'inch':  \n",
    "                        # Convert inches to pixels (dpi is dots per inch)  \n",
    "                        x_coords = [x * dpi for x in x_coords]  \n",
    "                        y_coords = [y * dpi for y in y_coords]  \n",
    "                    else:  \n",
    "                        print(f\"Unknown unit '{unit}' in page {page_number}\")  \n",
    "                        continue  \n",
    "\n",
    "                    x_min = min(x_coords)  \n",
    "                    x_max = max(x_coords)  \n",
    "                    y_min = min(y_coords)  \n",
    "                    y_max = max(y_coords)  \n",
    "\n",
    "                    # Ensure coordinates are within image bounds  \n",
    "                    left = int(max(x_min, 0))  \n",
    "                    upper = int(max(y_min, 0))  \n",
    "                    right = int(min(x_max, page_image.width))  \n",
    "                    lower = int(min(y_max, page_image.height))  \n",
    "\n",
    "                    # Crop the figure from the page image  \n",
    "                    figure_image = page_image.crop((left, upper, right, lower))  \n",
    "\n",
    "                    # Convert the image to bytes  \n",
    "                    image_buffer = BytesIO()  \n",
    "                    figure_image.save(image_buffer, format='PNG')  \n",
    "                    image_data = image_buffer.getvalue()  \n",
    "\n",
    "                    # Append the image data to the images list  \n",
    "                    images.append((blob.name, image_data, \"1\", page_number)) \n",
    "\n",
    "\n",
    "                    # Open the image from bytes  \n",
    "                    image = Image.open(io.BytesIO(image_data))  \n",
    "                    # Save the image to a bytes buffer in PNG format  \n",
    "                    buffered = io.BytesIO()  \n",
    "                    image.save(buffered, format=\"PNG\")  \n",
    "                    # Get the base64 encoded string  \n",
    "                    img_str = base64.b64encode(buffered.getvalue()).decode('utf-8')  \n",
    "                    # Create the base64 data URL  \n",
    "                    data_url = f\"data:image/png;base64,{img_str}\"  \n",
    "                \n",
    "                    embeddings_client = ImageEmbeddingsClient(  \n",
    "                                endpoint=ai_vision_endpoint,  \n",
    "                                credential=AzureKeyCredential(ai_vision_key),  \n",
    "                                model=ai_vision_model\n",
    "                    )  \n",
    "\n",
    "                    # Generate embeddings using Coheremebed  \n",
    "                    response = embeddings_client.embed(  \n",
    "                        input=[EmbeddingInput(image=data_url)]  \n",
    "                    )  \n",
    "                    print(\"Model:\", response.model)\n",
    "                    print(\"Usage:\", response.usage)\n",
    "                    print(response.data[0].embedding)\n",
    "                    break\n",
    "                break\n",
    "                    #display(figure_image)  \n",
    "   \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
