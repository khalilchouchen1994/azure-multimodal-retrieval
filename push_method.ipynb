{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Asynchronous Indexing Pipeline with Text and Image Embeddings  \n",
    "   \n",
    "This notebook demonstrates how to create a **custom asynchronous indexing pipeline** that:  \n",
    "   \n",
    "- Reads PDF documents from Azure Blob Storage.  \n",
    "- Extracts text and images using Azure Document Intelligence.  \n",
    "- Generates embeddings for text and images using **Cohere models** in **Azure AI Foundry**.  \n",
    "- Indexes the data into **Azure AI Search** with separate `text_vector` and `image_vector` fields.  \n",
    "- Allows searching over text and image vectors.  \n",
    "   \n",
    "We will go through the following steps:  \n",
    "   \n",
    "1. **Install Required Libraries**  \n",
    "2. **Set Up Environment Variables**  \n",
    "3. **Create the Azure AI Search Index**  \n",
    "4. **Define the Custom Indexing Pipeline Components**  \n",
    "5. **Initialize and Run the Indexing Pipeline**  \n",
    "6. **Perform Test Searches**  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configure logging \n",
    "To ensure a clear and structured output during the execution of this notebook, we configure logging at the start. This helps track the progress and debug any issues efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure logging for a clearer experience   \n",
    "import logging  \n",
    "   \n",
    "# Configure the root logger  \n",
    "logging.basicConfig(  \n",
    "    level=logging.INFO,  # Set root logger level  \n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'  \n",
    ")  \n",
    "   \n",
    "# Suppress logs from azure and uamqp libraries  \n",
    "logging.getLogger('azure').setLevel(logging.WARNING)  \n",
    "logging.getLogger('uamqp').setLevel(logging.WARNING)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Set Up Environment Variables  \n",
    "   \n",
    "Ensure that the `.env` file contains the following environment variables:  \n",
    "  \n",
    "- `AZURE_SEARCH_SERVICE_ENDPOINT`  \n",
    "- `AZURE_SEARCH_API_KEY`  \n",
    "- `AZURE_STORAGE_ACCOUNT_NAME`  \n",
    "- `AZURE_STORAGE_ACCOUNT_SUB_ID`  \n",
    "- `AZURE_STORAGE_ACCOUNT_RG_NAME`  \n",
    "- `AZURE_STORAGE_ACCOUNT_CONTAINER_NAME`  \n",
    "- `DOCUMENTINTELLIGENCE_ENDPOINT`  \n",
    "- `DOCUMENTINTELLIGENCE_API_KEY`  \n",
    "- `AZURE_AI_FOUNDRY_ENDPOINT`  \n",
    "- `AZURE_AI_FOUNDRY_KEY`  \n",
    "- `TEXT_EMBEDDING_MODEL` \n",
    "- `TEXT_EMBEDDING_DIMENSIONS` \n",
    "- `IMAGE_EMBEDDING_MODEL` \n",
    "- `IMAGE_EMBEDDING_DIMENSIONS` \n",
    "\n",
    "\n",
    "Avoid hardcoding sensitive information in the notebook. \n",
    "   \n",
    "Ensure that the identities used have the necessary permissions (e.g., **Storage Blob Data Reader** role).  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  \n",
    "from dotenv import load_dotenv  \n",
    "   \n",
    "# Load environment variables from .env file  \n",
    "load_dotenv(override=True)  \n",
    "   \n",
    "# Azure AI Search settings  \n",
    "search_service_endpoint = os.environ[\"AZURE_SEARCH_SERVICE_ENDPOINT\"]  \n",
    "search_api_key = os.environ[\"AZURE_SEARCH_API_KEY\"]  \n",
    "index_name = \"asynch-custom-push-demo\"  \n",
    "   \n",
    "# Azure Storage settings  \n",
    "storage_account_name = os.environ[\"AZURE_STORAGE_ACCOUNT_NAME\"]  \n",
    "storage_container_name = os.environ[\"AZURE_STORAGE_ACCOUNT_CONTAINER_NAME\"]  \n",
    "   \n",
    "# Azure AI Inference settings (for embeddings)  \n",
    "ai_foundry_endpoint = os.environ[\"AZURE_AI_FOUNDRY_ENDPOINT\"]  \n",
    "ai_foundry_key = os.environ[\"AZURE_AI_FOUNDRY_KEY\"] \n",
    "text_embedding_model = os.environ[\"TEXT_EMBEDDING_MODEL\"]\n",
    "text_embedding_dimensions = int(os.getenv(\"TEXT_EMBEDDING_DIMENSIONS\", 1024)) \n",
    "image_embedding_model= os.environ[\"IMAGE_EMBEDDING_MODEL\"]\n",
    "image_embedding_dimensions =  int(os.getenv(\"IMAGE_EMBEDDING_DIMENSIONS\", 1024))    # Set this based on your image embedding model  \n",
    "   \n",
    "# Azure Document Intelligence settings  \n",
    "document_intelligence_endpoint = os.environ[\"DOCUMENTINTELLIGENCE_ENDPOINT\"]  \n",
    "document_intelligence_key = os.environ[\"DOCUMENTINTELLIGENCE_API_KEY\"]  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Create the Azure AI Search Index  \n",
    "   \n",
    "We'll create the search index with the appropriate schema, including separate fields for `text_vector` and `image_vector`, and a `page_number` field to track the pages.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.search.documents.indexes import SearchIndexClient  \n",
    "from azure.search.documents.indexes.models import (  \n",
    "    SearchField,  \n",
    "    SearchFieldDataType,  \n",
    "    VectorSearch,  \n",
    "    HnswAlgorithmConfiguration,  \n",
    "    VectorSearchProfile,  \n",
    "    SemanticConfiguration,  \n",
    "    SemanticSearch,  \n",
    "    SemanticPrioritizedFields,  \n",
    "    SemanticField,  \n",
    "    SearchIndex  \n",
    ")  \n",
    "\n",
    "from azure.core.credentials import AzureKeyCredential  \n",
    "   \n",
    "# Create a SearchIndexClient  \n",
    "search_index_client = SearchIndexClient(  \n",
    "    endpoint=search_service_endpoint,  \n",
    "    credential=AzureKeyCredential(search_api_key)  \n",
    ")  \n",
    "   \n",
    "# Define the index schema  \n",
    "fields = [  \n",
    "    SearchField(  \n",
    "        name=\"parent_id\",  \n",
    "        type=SearchFieldDataType.String,  \n",
    "        filterable=True,  \n",
    "        facetable=True,  \n",
    "        sortable=True  \n",
    "    ),  \n",
    "    SearchField(  \n",
    "        name=\"chunk_id\",  \n",
    "        type=SearchFieldDataType.String,  \n",
    "        key=True,  \n",
    "        filterable=True,  \n",
    "        facetable=True,  \n",
    "        sortable=True  \n",
    "    ),  \n",
    "    SearchField(  \n",
    "        name=\"title\",  \n",
    "        type=SearchFieldDataType.String,  \n",
    "        filterable=True,  \n",
    "        facetable=True,  \n",
    "        sortable=True  \n",
    "    ),  \n",
    "    SearchField(  \n",
    "        name=\"chunk\",  \n",
    "        type=SearchFieldDataType.String,  \n",
    "        searchable=True  \n",
    "    ),  \n",
    "    SearchField(  \n",
    "        name=\"text_vector\",  \n",
    "        type=SearchFieldDataType.Collection(SearchFieldDataType.Single),  \n",
    "        vector_search_dimensions=text_embedding_dimensions,  \n",
    "        vector_search_profile_name=\"textHnswProfile\",  \n",
    "    ),  \n",
    "    SearchField(  \n",
    "        name=\"image_vector\",  \n",
    "        type=SearchFieldDataType.Collection(SearchFieldDataType.Single),  \n",
    "        vector_search_dimensions=image_embedding_dimensions,  \n",
    "        vector_search_profile_name=\"imageHnswProfile\",  \n",
    "    ),  \n",
    "    SearchField(  \n",
    "        name=\"page_number\",  \n",
    "        type=SearchFieldDataType.Int32,  \n",
    "        filterable=True,  \n",
    "        facetable=True,  \n",
    "        sortable=True  \n",
    "    ),  \n",
    "]  \n",
    "   \n",
    "# Configure the vector search settings  \n",
    "vector_search = VectorSearch(  \n",
    "    algorithms=[  \n",
    "        HnswAlgorithmConfiguration(name=\"myHnswAlgorithm\")  \n",
    "    ],  \n",
    "    profiles=[  \n",
    "        VectorSearchProfile(  \n",
    "            name=\"textHnswProfile\",  \n",
    "            algorithm_configuration_name=\"myHnswAlgorithm\",  \n",
    "        ),  \n",
    "        VectorSearchProfile(  \n",
    "            name=\"imageHnswProfile\",  \n",
    "            algorithm_configuration_name=\"myHnswAlgorithm\",  \n",
    "        ),  \n",
    "    ],  \n",
    ")  \n",
    "   \n",
    "# Configure semantic search settings (optional)  \n",
    "semantic_config = SemanticConfiguration(  \n",
    "    name=\"my-semantic-config\",  \n",
    "    prioritized_fields=SemanticPrioritizedFields(  \n",
    "        title_field=SemanticField(field_name=\"title\"),  \n",
    "        content_fields=[SemanticField(field_name=\"chunk\")],  \n",
    "    ),  \n",
    ")  \n",
    "   \n",
    "semantic_search = SemanticSearch(configurations=[semantic_config])  \n",
    "   \n",
    "# Create the search index  \n",
    "index = SearchIndex(  \n",
    "    name=index_name,  \n",
    "    fields=fields,  \n",
    "    vector_search=vector_search,  \n",
    "    semantic_search=semantic_search,  \n",
    ")  \n",
    "   \n",
    "# Create or update the index in Azure Cognitive Search  \n",
    "search_index_client.create_or_update_index(index)  \n",
    "   \n",
    "print(f\"Index '{index.name}' created or updated.\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Define the Custom Indexing Pipeline Components  \n",
    "   \n",
    "The custom indexing pipeline consists of the following components:  \n",
    "   \n",
    "- **FileReader**: Reads PDFs using Azure Document Intelligence and extracts text and images.  \n",
    "- **Chunker**: Splits the text into chunks for embedding.  \n",
    "- **TextEmbedder**: Generates text embeddings using Azure OpenAI.  \n",
    "- **ImageEmbedder**: Generates image embeddings using Azure AI Inference.  \n",
    "- **FileUploader**: Uploads the processed documents into Azure Cognitive Search.  \n",
    "- **AsynchronousIndexer**: Orchestrates the entire pipeline asynchronously.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Initialize and Run the Indexing Pipeline  \n",
    "   \n",
    "Now we'll initialize the `AsynchronousIndexer` with the appropriate settings and run the indexing pipeline. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio  \n",
    "import asyncio  \n",
    "from asynch_indexer.AsynchronousIndexer import AsynchronousIndexer  \n",
    "   \n",
    "# Necessary when running asyncio in Jupyter notebooks  \n",
    "nest_asyncio.apply()  \n",
    "   \n",
    "# Initialize the AsynchronousIndexer  \n",
    "indexer = AsynchronousIndexer(  \n",
    "    index_name=index_name,  \n",
    "    search_endpoint=search_service_endpoint,  \n",
    "    search_api_key=search_api_key,  \n",
    "    storage_account_name=storage_account_name,  \n",
    "    storage_container_name=storage_container_name,  \n",
    "    ai_foundry_endpoint = ai_foundry_endpoint  ,\n",
    "    ai_foundry_key = ai_foundry_key,\n",
    "    text_embedding_model= text_embedding_model,\n",
    "    image_embedding_model=image_embedding_model, \n",
    "    document_intelligence_endpoint=document_intelligence_endpoint,  \n",
    "    document_intelligence_key=document_intelligence_key,  \n",
    ")  \n",
    "   \n",
    "# Run the indexing pipeline  \n",
    "asyncio.run(indexer.run_indexing())  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Perform Test Searches  \n",
    "   \n",
    "Finally, we'll perform test searches against the index to verify that the text and image vectors have been indexed correctly.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.search.documents import SearchClient\n",
    "from azure.ai.inference import EmbeddingsClient   \n",
    "from azure.search.documents.models import VectorizedQuery   \n",
    "   \n",
    "# Initialize the SearchClient  \n",
    "search_client = SearchClient(  \n",
    "    endpoint=search_service_endpoint,  \n",
    "    index_name=index_name,  \n",
    "    credential=AzureKeyCredential(search_api_key),  \n",
    ")\n",
    "\n",
    "embeddings_client = EmbeddingsClient(  \n",
    "        endpoint=ai_foundry_endpoint,  \n",
    "        credential=AzureKeyCredential(ai_foundry_key),  \n",
    "        model=text_embedding_model\n",
    ")  \n",
    "   \n",
    "# Define the search query  \n",
    "query_text = \"Enter your search query here\"  \n",
    "   \n",
    "# Function to get the query embedding using Coheremebed\n",
    "def get_query_embedding(query):  \n",
    "    response = embeddings_client.embed(  \n",
    "        input=[query]  \n",
    "    )  \n",
    "    print(\"Model:\", response.model)\n",
    "    print(\"Usage:\", response.usage)\n",
    "    return response.data[0].embedding\n",
    "   \n",
    "# Get the query embedding  \n",
    "query_embedding = get_query_embedding(query_text) \n",
    "   \n",
    "# Perform the vector search  \n",
    "vector_query = VectorizedQuery(  \n",
    "    vector=query_embedding,  \n",
    "    k_nearest_neighbors=3,  \n",
    "    fields=\"text_vector\", \n",
    ")  \n",
    "   \n",
    "results = search_client.search(  \n",
    "    search_text=query_embedding,  \n",
    "    vector_queries=[vector_query],  \n",
    "    select=[\"title\", \"chunk\", \"page_number\"],  \n",
    "    top=3  \n",
    ")  \n",
    "   \n",
    "# Print the results  \n",
    "for result in results:  \n",
    "    print(f\"Title: {result['title']}\")  \n",
    "    print(f\"Page Number: {result['page_number']}\")  \n",
    "    print(f\"Chunk: {result['chunk']}\")  \n",
    "    print(\"---\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion  \n",
    "   \n",
    "In this notebook, we've built a custom asynchronous indexing pipeline that processes both text and images from PDF documents, generates embeddings using Azure OpenAI and Azure AI Inference, and indexes them into Azure Cognitive Search. This allows for advanced vector-based searches over both text and images.  \n",
    "   \n",
    "You can extend this pipeline to handle more complex scenarios, larger datasets, or integrate additional processing steps as needed.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
